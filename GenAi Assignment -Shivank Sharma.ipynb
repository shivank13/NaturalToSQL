{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23ba022",
   "metadata": {},
   "source": [
    "# <b>Application code to convert natural language to sql query</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24db7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12c917",
   "metadata": {},
   "source": [
    " **Creating a prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71489844",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a chatbot that will interact with a human,you only have to answer to the question relating\n",
    "to queries.\n",
    "\n",
    "Given the following context and a human input,Convert the input to Sql query using the context that will\n",
    "have database schema or\n",
    "If the question cannot be answered using the information \n",
    "provided answer with \"I don't know\".\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ab054",
   "metadata": {},
   "source": [
    "**Creating memory and initialising LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03b061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\", \"context\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "chain =LLMChain(\n",
    "    llm=ChatOpenAI(openai_api_key=\"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\",temperature=0)\n",
    "    , memory=memory, prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a190318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    import tiktoken\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "#function to order relevant chunks\n",
    "def rank_texts(relevant_texts, query, threshold=3, max_tokens=2700):\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    import numpy as np\n",
    "    # Model 1: cross-encoder/stsb-roberta-large\n",
    "    # Model 2: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "    model_encoder = CrossEncoder(\"cross-encoder/stsb-roberta-large\")\n",
    "    relevant_context = \"\"\n",
    "    query_paras_combined = [[query, para] for para in relevant_texts]\n",
    "    similarity_scores = model_encoder.predict(query_paras_combined)\n",
    "    sim_scores_argsort = list(reversed(np.argsort(similarity_scores)))\n",
    "    for idx in sim_scores_argsort:\n",
    "#         print(\"{:.2f}\\t{}\".format(similarity_scores[idx], relevant_texts[idx]))\n",
    "\n",
    "#         print('\\n********************************************\\n\\n')\n",
    "        if threshold > 0 and num_tokens_from_string(relevant_context, \"p50k_base\") + num_tokens_from_string(\n",
    "                relevant_texts[idx], \"p50k_base\") < max_tokens:\n",
    "            relevant_context += relevant_texts[idx] + \"\\n\\n\"\n",
    "            threshold = threshold - 1\n",
    "        else:\n",
    "            break\n",
    "    return relevant_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f4eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split large texts into chunks\n",
    "def split_text(text, chunk_size=200, chunk_overlap=30):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = splitter.split_text(text)\n",
    "    return texts\n",
    "\n",
    "#function to get embeddings of texts\n",
    "def get_embeddings(texts):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # Model 1: msmarco-distilbert-base-v4 \n",
    "    # Model 2: all-MiniLM-L6-v2\n",
    "    model_vector = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    embeddings = model_vector.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "#function to index the embeddings\n",
    "def index_embeddings(embeddings):\n",
    "    import faiss\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "#function to clean texts\n",
    "def clean_text(text):\n",
    "    cleaned_string = text.replace(\"\\n\",\"\").replace('..',\"\")\n",
    "    return cleaned_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ae200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "#function to count tokens on openai calls\n",
    "def count_tokens(context,query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result=chain({\"context\":context , \"human_input\": query},return_only_outputs=True)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide the context(database schema) and the query in natural language, the application\n",
      "      will convert it into sql query\n",
      "Input the context(database schema)a table named students having columns names,class,percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivanksharma4/anaconda3/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the statement in natural language who is virat?\n",
      "Spent a total of 126 tokens\n",
      "I don't know.\n",
      "Input the statement in natural language shbcsdka\n",
      "Spent a total of 141 tokens\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "print('''Provide the context(database schema) and the query in natural language, the application\n",
    "      will convert it into sql query''')\n",
    "while True:\n",
    "    context= input(\"Input the context(database schema)\")\n",
    "    if not context:\n",
    "        break\n",
    "    text_chunks = split_text(context)\n",
    "    cleaned_context = [clean_text(para) for para in text_chunks]\n",
    "    embedded_context=get_embeddings(cleaned_context)\n",
    "    \n",
    "    index = index_embeddings(embedded_context)\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Input the statement in natural language \")\n",
    "        if not query:\n",
    "            break\n",
    "        query_vec =get_embeddings([query])\n",
    "        k=5\n",
    "        D,I = index.search(query_vec, k)\n",
    "        relevant_indexes = I.tolist()[0]\n",
    "\n",
    "        #fetching the relevant chunks\n",
    "        relevant_chunks = []\n",
    "        for i in relevant_indexes:\n",
    "            relevant_chunks.append(text_chunks[i]) \n",
    "        context = rank_texts(relevant_chunks, query) \n",
    "        print(count_tokens(context,query)['text'])\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b087e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.memory.buffer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
